{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "S9o26QJOBdIc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9o26QJOBdIc",
    "outputId": "2c1cde5f-5efb-4a7c-c158-f462eaa6f2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/flavia-dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"gauravneupane/flavia-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oy1NfeRpGSNo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oy1NfeRpGSNo",
    "outputId": "938cc9f6-10b5-4d41-9178-d640bf83549a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, google-pasta, tensorboard, astunparse, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 protobuf-5.29.5 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "fcce04bdd13649c3a6aeeac0073010d3",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6e7750-6d34-40f7-8277-e3e28071c03d",
   "metadata": {
    "id": "3b6e7750-6d34-40f7-8277-e3e28071c03d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0045d27b-b870-4ad7-b542-c44c6d0e0202",
   "metadata": {
    "id": "0045d27b-b870-4ad7-b542-c44c6d0e0202"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "\n",
    "def load_flavia_dataset_from_csv(csv_path, img_size, base_image_dir=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess Flavia dataset from a CSV file with robust error handling\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing image paths and labels\n",
    "    img_size : tuple\n",
    "        Desired image size (width, height)\n",
    "    base_image_dir : str, optional\n",
    "        Base directory for image files if paths in CSV are relative\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array\n",
    "        Preprocessed image data\n",
    "    y : numpy array\n",
    "        Integer labels\n",
    "    y_onehot : numpy array\n",
    "        One-hot encoded labels\n",
    "    class_names : list\n",
    "        List of unique class names\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    X = []  # Images\n",
    "    y = []  # Labels\n",
    "\n",
    "    # Ensure we have the correct columns\n",
    "    if 'id' not in df.columns or 'y' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'image path' and 'y' columns\")\n",
    "\n",
    "    # Track unique classes for naming\n",
    "    class_names = df['y'].unique().tolist()\n",
    "\n",
    "    # Iterate through rows in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Construct full path if base_image_dir is provided\n",
    "        img_path = row['id']\n",
    "        if base_image_dir:\n",
    "            img_path = os.path.join(base_image_dir, img_path)\n",
    "\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"File not found: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Open image with PIL\n",
    "            with Image.open(img_path) as img:\n",
    "                # Convert to RGB if not already\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "                # Resize image\n",
    "                img = img.resize(img_size, Image.LANCZOS)\n",
    "\n",
    "                # Convert to numpy array and normalize\n",
    "                img_array = np.array(img) / 255.0\n",
    "\n",
    "                X.append(img_array)\n",
    "                y.append(row['y'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Check if any images were loaded\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No images could be loaded. Check file paths and image formats.\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    num_classes = len(np.unique(y))\n",
    "    y_onehot = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "    print(f\"Loaded {len(X)} images\")\n",
    "    return X, y, y_onehot, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52663a9f-356f-4c9e-9075-34b752256637",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52663a9f-356f-4c9e-9075-34b752256637",
    "outputId": "37d0902b-1e68-4194-a046-19fab95808c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1907 images\n"
     ]
    }
   ],
   "source": [
    "X,y,y_onehot, class_names = load_flavia_dataset_from_csv(csv_path ='/kaggle/input/flavia-dataset/Leaves/all.csv', img_size=(256,256), base_image_dir='/kaggle/input/flavia-dataset/Leaves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302342b2-dc7a-4b3d-a0d8-37d5fab4e718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "302342b2-dc7a-4b3d-a0d8-37d5fab4e718",
    "outputId": "f8eca5b0-8066-4e06-921c-81ecef73b44d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1907, 256, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abecc92a-4d68-4b87-9ad0-64d548b1cb95",
   "metadata": {
    "id": "abecc92a-4d68-4b87-9ad0-64d548b1cb95"
   },
   "outputs": [],
   "source": [
    " # Split data into training + validation set and test set (e.g., 80% train/val, 20% test)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    " X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    "     )\n",
    "\n",
    "# Split the training + validation set into training and validation sets (e.g., 80% train, 20% val from the 80% train/val set)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    " X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89fa306-c035-4356-902a-cc9ab9b339a1",
   "metadata": {
    "id": "a89fa306-c035-4356-902a-cc9ab9b339a1"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6058db68-4902-4fc0-a151-a9a5fd0b45e6",
   "metadata": {
    "id": "6058db68-4902-4fc0-a151-a9a5fd0b45e6"
   },
   "outputs": [],
   "source": [
    "def create_xception_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using Xception as the base\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input images (height, width, channels)\n",
    "    num_classes : int\n",
    "        Number of classes to classify\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Model : tensorflow.keras.Model\n",
    "        Compiled transfer learning model\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pre-trained Xception model\n",
    "    base_model = Xception(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68054895-328c-46fc-9678-7ba430789cec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68054895-328c-46fc-9678-7ba430789cec",
    "outputId": "b0df8f17-7470-4091-cc4f-e3a1015d5f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 9s/step - accuracy: 0.0900 - loss: 3.3995 - val_accuracy: 0.5836 - val_loss: 2.6229 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m 1/38\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2.8098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2.8098 - val_accuracy: 0.5836 - val_loss: 2.5992 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 9s/step - accuracy: 0.3779 - loss: 2.6870 - val_accuracy: 0.7016 - val_loss: 1.7991 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.5625 - loss: 2.1765 - val_accuracy: 0.7115 - val_loss: 1.7781 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 9s/step - accuracy: 0.5460 - loss: 2.0357 - val_accuracy: 0.7934 - val_loss: 1.2021 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.6875 - loss: 1.6649 - val_accuracy: 0.7836 - val_loss: 1.1904 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 9s/step - accuracy: 0.6182 - loss: 1.5356 - val_accuracy: 0.8590 - val_loss: 0.8341 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.7188 - loss: 1.1771 - val_accuracy: 0.8557 - val_loss: 0.8268 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 9s/step - accuracy: 0.6857 - loss: 1.2100 - val_accuracy: 0.8689 - val_loss: 0.6267 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.5312 - loss: 1.2363 - val_accuracy: 0.8689 - val_loss: 0.6225 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 9s/step - accuracy: 0.7431 - loss: 0.9881 - val_accuracy: 0.9180 - val_loss: 0.4731 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.6250 - loss: 1.3415 - val_accuracy: 0.9148 - val_loss: 0.4689 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 9s/step - accuracy: 0.7495 - loss: 0.8689 - val_accuracy: 0.9311 - val_loss: 0.3896 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.6562 - loss: 1.0433 - val_accuracy: 0.9311 - val_loss: 0.3871 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 9s/step - accuracy: 0.8011 - loss: 0.7669 - val_accuracy: 0.9377 - val_loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.8438 - loss: 0.7605 - val_accuracy: 0.9377 - val_loss: 0.3311 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 9s/step - accuracy: 0.8296 - loss: 0.6519 - val_accuracy: 0.9541 - val_loss: 0.2755 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.6875 - loss: 0.8779 - val_accuracy: 0.9508 - val_loss: 0.2762 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 9s/step - accuracy: 0.8427 - loss: 0.5950 - val_accuracy: 0.9443 - val_loss: 0.2508 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.6759 - val_accuracy: 0.9475 - val_loss: 0.2508 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 9s/step - accuracy: 0.8449 - loss: 0.5242 - val_accuracy: 0.9541 - val_loss: 0.2246 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 2s/step - accuracy: 0.8125 - loss: 0.6390 - val_accuracy: 0.9541 - val_loss: 0.2228 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    " # Create and compile the model\n",
    "model = create_xception_transfer_learning_model(\n",
    "        input_shape=X.shape[1:],\n",
    "        num_classes=y_onehot.shape[1]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=32),\n",
    "        epochs=30,\n",
    "        validation_data=(X_val, y_val),\n",
    "        steps_per_epoch=len(X_train) // 32,\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5f80c1-de54-4a34-ae25-0e72790701e7",
   "metadata": {
    "id": "5d5f80c1-de54-4a34-ae25-0e72790701e7"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance on the test set\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tensorflow.keras.Model\n",
    "        Trained model\n",
    "    X_test : numpy array\n",
    "        Test images\n",
    "    y_test : numpy array\n",
    "        One-hot encoded test labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Prints model evaluation metrics\n",
    "    \"\"\"\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Predict and generate a classification report\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4tvDbGZ6DnKa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tvDbGZ6DnKa",
    "outputId": "3d270d42-5a10-48bd-d4cf-bba4620b4bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 0.9145 - loss: 0.3650\n",
      "\n",
      "Final Test Loss: 0.3313\n",
      "Final Test Accuracy: 92.15%\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7s/step\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.91      0.77      0.83        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       1.00      1.00      1.00        15\n",
      "           5       1.00      1.00      1.00        11\n",
      "           6       0.90      0.75      0.82        12\n",
      "           7       0.91      1.00      0.95        10\n",
      "           8       0.82      0.82      0.82        11\n",
      "           9       0.92      0.92      0.92        12\n",
      "          10       1.00      0.80      0.89        10\n",
      "          11       1.00      0.92      0.96        13\n",
      "          12       1.00      0.80      0.89        10\n",
      "          13       0.92      0.92      0.92        13\n",
      "          14       0.92      1.00      0.96        12\n",
      "          15       1.00      1.00      1.00        11\n",
      "          16       1.00      1.00      1.00        16\n",
      "          17       1.00      1.00      1.00        12\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.86      0.92      0.89        13\n",
      "          20       1.00      1.00      1.00        12\n",
      "          21       0.85      1.00      0.92        11\n",
      "          22       0.73      1.00      0.85        11\n",
      "          23       0.87      1.00      0.93        13\n",
      "          24       1.00      0.73      0.84        11\n",
      "          25       0.89      0.80      0.84        10\n",
      "          26       1.00      0.73      0.84        11\n",
      "          27       0.92      1.00      0.96        11\n",
      "          28       0.83      0.91      0.87        11\n",
      "          29       0.65      1.00      0.79        13\n",
      "          30       1.00      0.82      0.90        11\n",
      "          31       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.92       382\n",
      "   macro avg       0.93      0.92      0.92       382\n",
      "weighted avg       0.93      0.92      0.92       382\n",
      "\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "    # Evaluate the model\n",
    "evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Optional: Save the model\n",
    "model.save('xception_leaf_classification_model.keras')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MZSoLPFIVhNk",
   "metadata": {
    "id": "MZSoLPFIVhNk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 08:44:16.736660: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18762d73-a45a-45b1-82bf-afd76414fac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InputLayer name=input_layer, built=True>,\n",
       " <Conv2D name=block1_conv1, built=True>,\n",
       " <BatchNormalization name=block1_conv1_bn, built=True>,\n",
       " <Activation name=block1_conv1_act, built=True>,\n",
       " <Conv2D name=block1_conv2, built=True>,\n",
       " <BatchNormalization name=block1_conv2_bn, built=True>,\n",
       " <Activation name=block1_conv2_act, built=True>,\n",
       " <SeparableConv2D name=block2_sepconv1, built=True>,\n",
       " <BatchNormalization name=block2_sepconv1_bn, built=True>,\n",
       " <Activation name=block2_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block2_sepconv2, built=True>,\n",
       " <BatchNormalization name=block2_sepconv2_bn, built=True>,\n",
       " <Conv2D name=conv2d, built=True>,\n",
       " <MaxPooling2D name=block2_pool, built=True>,\n",
       " <BatchNormalization name=batch_normalization, built=True>,\n",
       " <Add name=add, built=True>,\n",
       " <Activation name=block3_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block3_sepconv1, built=True>,\n",
       " <BatchNormalization name=block3_sepconv1_bn, built=True>,\n",
       " <Activation name=block3_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block3_sepconv2, built=True>,\n",
       " <BatchNormalization name=block3_sepconv2_bn, built=True>,\n",
       " <Conv2D name=conv2d_1, built=True>,\n",
       " <MaxPooling2D name=block3_pool, built=True>,\n",
       " <BatchNormalization name=batch_normalization_1, built=True>,\n",
       " <Add name=add_1, built=True>,\n",
       " <Activation name=block4_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block4_sepconv1, built=True>,\n",
       " <BatchNormalization name=block4_sepconv1_bn, built=True>,\n",
       " <Activation name=block4_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block4_sepconv2, built=True>,\n",
       " <BatchNormalization name=block4_sepconv2_bn, built=True>,\n",
       " <Conv2D name=conv2d_2, built=True>,\n",
       " <MaxPooling2D name=block4_pool, built=True>,\n",
       " <BatchNormalization name=batch_normalization_2, built=True>,\n",
       " <Add name=add_2, built=True>,\n",
       " <Activation name=block5_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block5_sepconv1, built=True>,\n",
       " <BatchNormalization name=block5_sepconv1_bn, built=True>,\n",
       " <Activation name=block5_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block5_sepconv2, built=True>,\n",
       " <BatchNormalization name=block5_sepconv2_bn, built=True>,\n",
       " <Activation name=block5_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block5_sepconv3, built=True>,\n",
       " <BatchNormalization name=block5_sepconv3_bn, built=True>,\n",
       " <Add name=add_3, built=True>,\n",
       " <Activation name=block6_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block6_sepconv1, built=True>,\n",
       " <BatchNormalization name=block6_sepconv1_bn, built=True>,\n",
       " <Activation name=block6_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block6_sepconv2, built=True>,\n",
       " <BatchNormalization name=block6_sepconv2_bn, built=True>,\n",
       " <Activation name=block6_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block6_sepconv3, built=True>,\n",
       " <BatchNormalization name=block6_sepconv3_bn, built=True>,\n",
       " <Add name=add_4, built=True>,\n",
       " <Activation name=block7_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block7_sepconv1, built=True>,\n",
       " <BatchNormalization name=block7_sepconv1_bn, built=True>,\n",
       " <Activation name=block7_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block7_sepconv2, built=True>,\n",
       " <BatchNormalization name=block7_sepconv2_bn, built=True>,\n",
       " <Activation name=block7_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block7_sepconv3, built=True>,\n",
       " <BatchNormalization name=block7_sepconv3_bn, built=True>,\n",
       " <Add name=add_5, built=True>,\n",
       " <Activation name=block8_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block8_sepconv1, built=True>,\n",
       " <BatchNormalization name=block8_sepconv1_bn, built=True>,\n",
       " <Activation name=block8_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block8_sepconv2, built=True>,\n",
       " <BatchNormalization name=block8_sepconv2_bn, built=True>,\n",
       " <Activation name=block8_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block8_sepconv3, built=True>,\n",
       " <BatchNormalization name=block8_sepconv3_bn, built=True>,\n",
       " <Add name=add_6, built=True>,\n",
       " <Activation name=block9_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block9_sepconv1, built=True>,\n",
       " <BatchNormalization name=block9_sepconv1_bn, built=True>,\n",
       " <Activation name=block9_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block9_sepconv2, built=True>,\n",
       " <BatchNormalization name=block9_sepconv2_bn, built=True>,\n",
       " <Activation name=block9_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block9_sepconv3, built=True>,\n",
       " <BatchNormalization name=block9_sepconv3_bn, built=True>,\n",
       " <Add name=add_7, built=True>,\n",
       " <Activation name=block10_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block10_sepconv1, built=True>,\n",
       " <BatchNormalization name=block10_sepconv1_bn, built=True>,\n",
       " <Activation name=block10_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block10_sepconv2, built=True>,\n",
       " <BatchNormalization name=block10_sepconv2_bn, built=True>,\n",
       " <Activation name=block10_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block10_sepconv3, built=True>,\n",
       " <BatchNormalization name=block10_sepconv3_bn, built=True>,\n",
       " <Add name=add_8, built=True>,\n",
       " <Activation name=block11_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block11_sepconv1, built=True>,\n",
       " <BatchNormalization name=block11_sepconv1_bn, built=True>,\n",
       " <Activation name=block11_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block11_sepconv2, built=True>,\n",
       " <BatchNormalization name=block11_sepconv2_bn, built=True>,\n",
       " <Activation name=block11_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block11_sepconv3, built=True>,\n",
       " <BatchNormalization name=block11_sepconv3_bn, built=True>,\n",
       " <Add name=add_9, built=True>,\n",
       " <Activation name=block12_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block12_sepconv1, built=True>,\n",
       " <BatchNormalization name=block12_sepconv1_bn, built=True>,\n",
       " <Activation name=block12_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block12_sepconv2, built=True>,\n",
       " <BatchNormalization name=block12_sepconv2_bn, built=True>,\n",
       " <Activation name=block12_sepconv3_act, built=True>,\n",
       " <SeparableConv2D name=block12_sepconv3, built=True>,\n",
       " <BatchNormalization name=block12_sepconv3_bn, built=True>,\n",
       " <Add name=add_10, built=True>,\n",
       " <Activation name=block13_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block13_sepconv1, built=True>,\n",
       " <BatchNormalization name=block13_sepconv1_bn, built=True>,\n",
       " <Activation name=block13_sepconv2_act, built=True>,\n",
       " <SeparableConv2D name=block13_sepconv2, built=True>,\n",
       " <BatchNormalization name=block13_sepconv2_bn, built=True>,\n",
       " <Conv2D name=conv2d_3, built=True>,\n",
       " <MaxPooling2D name=block13_pool, built=True>,\n",
       " <BatchNormalization name=batch_normalization_3, built=True>,\n",
       " <Add name=add_11, built=True>,\n",
       " <SeparableConv2D name=block14_sepconv1, built=True>,\n",
       " <BatchNormalization name=block14_sepconv1_bn, built=True>,\n",
       " <Activation name=block14_sepconv1_act, built=True>,\n",
       " <SeparableConv2D name=block14_sepconv2, built=True>,\n",
       " <BatchNormalization name=block14_sepconv2_bn, built=True>,\n",
       " <Activation name=block14_sepconv2_act, built=True>,\n",
       " <GlobalAveragePooling2D name=global_average_pooling2d, built=True>,\n",
       " <Dense name=dense, built=True>,\n",
       " <Dropout name=dropout, built=True>,\n",
       " <Dense name=dense_1, built=True>,\n",
       " <Dropout name=dropout_1, built=True>,\n",
       " <Dense name=dense_2, built=True>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7841b0-b97e-4728-8d62-46f7fbc652b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
